{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "from nltk import sent_tokenize, RegexpParser, pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "429"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = \"ID:             \"\n",
    "to_find = \"TARGET:\"\n",
    "\n",
    "quote_pattern = re.compile('\".*?\"')\n",
    "\n",
    "id_stuff = re.compile('\\(.*?\\)')\n",
    "\n",
    "def read_answers(): \n",
    "    answers_dict = {}\n",
    "    data_file = os.path.abspath(\"merged_answers_ips.txt\")\n",
    "\n",
    "    with open(data_file, 'r') as f:\n",
    "        content = f.readlines()\n",
    "    curr_id = \"\"\n",
    "    for line in content:\n",
    "        if id in line:\n",
    "            line = line.replace(id,'').strip()\n",
    "            stuff = id_stuff.findall(line)\n",
    "            for s in stuff:\n",
    "                line = line.replace(s,'')\n",
    "            curr_id = line.strip()\n",
    "        elif to_find in line: \n",
    "            line_split = [x.strip() for x in line.split(':')]\n",
    "            matches = line_split[1]\n",
    "            if curr_id not in answers_dict.keys():\n",
    "                answers_dict[curr_id] = matches\n",
    "            else:\n",
    "                answers_dict[curr_id].extend(matches)\n",
    "    return answers_dict\n",
    "\n",
    "\n",
    "ans_dict = read_answers()\n",
    "len(ans_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DEV-MUC3-0004', 'DEV-MUC3-0006', 'DEV-MUC3-0012', 'DEV-MUC3-0014', 'DEV-MUC3-0022', 'DEV-MUC3-0023', 'DEV-MUC3-0025', 'DEV-MUC3-0026', 'DEV-MUC3-0028', 'DEV-MUC3-0033']\n"
     ]
    }
   ],
   "source": [
    "# DEV-MUC3-0001 (NOSC)\n",
    "\n",
    "text_files = os.path.abspath('merged_project_ips.txt')\n",
    "id_match = re.compile('[A-Z]{3}[1-9]*-[A-Z]{3}[1-9]*-[0-9]{4}')\n",
    "id_stuff = re.compile('\\(.*?\\)')\n",
    "\n",
    "with open(text_files, 'r') as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "content = [c.strip() for c in content]\n",
    "\n",
    "file_dict = {}    \n",
    "\n",
    "curr_file_id = \"\"\n",
    "file = []\n",
    "for line in content:\n",
    "    if id_match.match(line):\n",
    "        if len(file) > 0:\n",
    "            file = \" \".join(file)\n",
    "            file_dict[curr_file_id] = file\n",
    "            file = []\n",
    "        stuff = id_stuff.findall(line)\n",
    "        for s in stuff:\n",
    "            line = line.replace(s,'')\n",
    "        curr_file_id = line.strip()\n",
    "    else:\n",
    "        file.append(line)\n",
    "        \n",
    "len(file_dict)\n",
    "print(list(file_dict)[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added\nadded\nadded\nadded\nadded"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nadded\nadded"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\nadded\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-2a08989132eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mans\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'added'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                     \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"(TARGET)\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"(TARGET)\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "sentences = []\n",
    "ol = len(sentences)\n",
    "for id in file_dict:\n",
    "    if id in ans_dict.keys():\n",
    "        file = file_dict[id]\n",
    "        answers = ans_dict[id]\n",
    "        for sent in sent_tokenize(file):\n",
    "            for ans in answers:\n",
    "                if ans in sent:\n",
    "                    sent = sent.replace(ans, \"\".join([ans,\"(TARGET)\"]))\n",
    "            if \"(TARGET)\" in sent:\n",
    "                sentences.append(sent)\n",
    "        if len(sentences) > ol:\n",
    "            sentences.append('\\n#########################################################\\n')\n",
    "            ol = len(sentences)\n",
    "\n",
    "print(len(sentences))\n",
    "sentences = \"\\n\\n\".join(sentences)\n",
    "\n",
    "with open('target_sent.txt', 'w') as f:\n",
    "    f.writelines(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = os.path.abspath('merged_file.txt')\n",
    "with open(input_data, 'r') as f:\n",
    "    read_content = f.read()\n",
    "content = [c.replace('\\n', ' ') for c in sent_tokenize(read_content)]\n",
    "read_content = read_content.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_rules = [\n",
    "    '((\\\\bclaim[^\\b]+\\\\b|\\\\baccept[^\\b]+\\\\b).*(\\\\bresponsibility\\\\b))', # perp claimed/accepted responsibility\n",
    "    '((\\\\bha[^\\b]+\\\\b)+ *(\\\\bkilled\\\\b))', # perp has killed\n",
    "    '(were +arrested)' # perp were arrested\n",
    "    '(blamed) +' # blamed _perp\n",
    "    \"massacred\",\n",
    "    'kidnapped',\n",
    "    'kidnapped by',\n",
    "    'carried out by',\n",
    "    'carried out',\n",
    "    '(killed by)' # killed by. does not perform well\n",
    "    '(^(we[^\\b]+) *destroyed)'\n",
    "    '(attacked)',\n",
    "    \"guerrilla['s]? of\",\n",
    "    \"member['s]? of\",\n",
    "    'threatened',\n",
    "    'ambushed'\n",
    "]\n",
    "\n",
    "matched_sents = []\n",
    "\n",
    "test_content = content[0:15000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DEV-MUC3-0001', 196), ('COPREFA', 171), ('THE ARCE BATTALION', 166), ('SAN MIGUEL DEPARTMENT', 137)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('SAN MIGUEL DEPARTMENT', 137)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perp_org(text):\n",
    "    for line in text:\n",
    "        b = line.lower()\n",
    "        for r in regex_rules:\n",
    "            a = re.findall(r, b)\n",
    "            if len(a) > 0:\n",
    "                sent = line\n",
    "                line_parse = nlp(sent)\n",
    "                orgs = [x for x in line_parse.ents if x.label_ == \"ORG\"]\n",
    "                if len(orgs) > 0:\n",
    "                    orgs = [(x.text, x.start) for x in orgs]\n",
    "                    reg_pos = line.lower().find(a[0][0])\n",
    "                    orgs = [(x[0], reg_pos - x[1]) for x in orgs]\n",
    "                    print(orgs)\n",
    "                    return min(orgs, key=lambda x: x[1])\n",
    "               \n",
    "               \n",
    "def perp_ind(text):\n",
    "    for line in text:\n",
    "        b = line.lower()\n",
    "        for r in regex_rules:\n",
    "            a = re.findall(r, b)\n",
    "            if len(a) > 0:\n",
    "                sent = line\n",
    "                line_parse = nlp(sent)\n",
    "                inds = [x for x in line_parse.ents if x.label_ == 'PERSON']\n",
    "                if len(inds) > 0:\n",
    "                    inds = [(x.text, x.start) for x in inds]\n",
    "                    reg_pos = line.lower().find(a[0][0])\n",
    "                    inds = [(x[0], reg_pos - x[1]) for x in inds]\n",
    "                    return min(inds, key=lambda x:x[1])\n",
    "                \n",
    "\n",
    "perp_org(test_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-193-f069aac52ce1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweaponDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdifflib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequenceMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mcorrel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcorrel\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m90\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0;31m#print \"Before:\", w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/difflib.py\u001b[0m in \u001b[0;36mratio\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \"\"\"\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtriple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_calculate_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/difflib.py\u001b[0m in \u001b[0;36mget_matching_blocks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0malo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mahi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbhi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_longest_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mahi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbhi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m             \u001b[0;31m# a[alo:i] vs b[blo:j] unknown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0;31m# a[i:i+k] same as b[j:j+k]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/difflib.py\u001b[0m in \u001b[0;36mfind_longest_match\u001b[0;34m(self, alo, ahi, blo, bhi)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;31m# look at all instances of a[i] in b; note that because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;31m# b2j has no junk keys, the loop is skipped if a[i] is junk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mj2lenget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj2len\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0mnewj2len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb2j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#  Shuvrajit's  Code\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Nov 05 18:56:36 2017\n",
    "\n",
    "@author: Shuvrajit\n",
    "\"\"\"\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, ne_chunk\n",
    "import os\n",
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#from sklearn.cluster import KMeans\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import difflib\n",
    "from nltk import RegexpParser, Tree\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import random\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def getFeatures(answerfolder, textfolder):\n",
    "    example = [] #stores the document\n",
    "    label = [] #stores corresponding label\n",
    "    for files in os.listdir(textfolder):\n",
    "        #print files\n",
    "        ansextension = \"\".join([files,'.anskey'])\n",
    "        textpath = os.path.join(textfolder,files)\n",
    "        anspath = os.path.join(answerfolder,ansextension)\n",
    "        ftext = open(textpath)\n",
    "        flabel = open(anspath)\n",
    "        text = ftext.read()\n",
    "        example.append(text)\n",
    "        answer = flabel.read()\n",
    "        answer = answer.split('\\n')\n",
    "        for slots in answer:\n",
    "            slot = slots.split(':')\n",
    "            if slot[0] == 'INCIDENT':\n",
    "                l = slot[-1].strip()            \n",
    "                lab = incident[l]\n",
    "                label.append(lab)\n",
    "        ftext.close()\n",
    "        flabel.close()\n",
    "    return example, label\n",
    "\n",
    "\n",
    "# Import re as above. Compile a regex expression with string below\n",
    "p = re.compile('[A-Z]{3}[1-9]*-[A-Z]{3}[1-9]*-[0-9]{4}')\n",
    "\n",
    "#Creating a test file\n",
    "\n",
    "testfile = 'news.txt' #Text\n",
    "testfileAns = 'news.txt.ans' #Gold Template\n",
    "\n",
    "f = open(testfile, 'w')\n",
    "f1 = open(testfileAns, 'w')\n",
    "for files in os.listdir('testSet/text'):    \n",
    "    textpath = os.path.join('testSet/text',files)\n",
    "    ansext = \"\".join([files,'.anskey'])\n",
    "    anspath = os.path.join('testSet/answers',ansext)\n",
    "    ftext = open(textpath)    \n",
    "    fans = open(anspath)\n",
    "    text = ftext.read()\n",
    "    ans = fans.read()\n",
    "    f.write(text)\n",
    "    f.write('\\n')\n",
    "    f1.write(ans)\n",
    "    f1.write('\\n')\n",
    "f.close()\n",
    "f1.close()\n",
    "\n",
    "#Reading the test file \n",
    "#and converting into list of documents\n",
    "corpus = []\n",
    "document = []\n",
    "header = []\n",
    "with open(\"news.txt\", 'r') as f:\n",
    "    for sent in f.readlines():    \n",
    "        sent1 = sent.split('\\n')\n",
    "        if p.match(sent1[0]) is None:\n",
    "            document.append(sent.lower())        \n",
    "        else:\n",
    "            corpus.append(\"\".join(document))\n",
    "            header.append(sent1[0])\n",
    "            document = [sent.lower()]  \n",
    "corpus.append(\"\".join(document))\n",
    "\n",
    "#Incident Extraction\n",
    "incident = {'ARSON':1, 'ATTACK':2, 'BOMBING':3, 'KIDNAPPING':4, 'ROBBERY':5}\n",
    "revinc = {1:'ARSON', 2:'ATTACK', 3:'BOMBING', 4:'KIDNAPPING', 5:'ROBBERY'}\n",
    "\n",
    "#Training the classifier for incident extraction\n",
    "answerfolder = 'developset/answers'\n",
    "textfolder = 'developset/texts'\n",
    "featureRaw = getFeatures(answerfolder,textfolder)\n",
    "x_train, y_train = featureRaw[0], featureRaw[-1]\n",
    "countVec = TfidfVectorizer(stop_words = 'english')\n",
    "#Creating tf-idf vector for the documents\n",
    "x_trainCV = countVec.fit_transform(x_train)\n",
    "\n",
    "answerfolder = 'testSet/answers'\n",
    "textfolder = 'testSet/text'            \n",
    "featureRaw = getFeatures(answerfolder,textfolder)\n",
    "x_test, y_test = featureRaw[0], featureRaw[-1]\n",
    "x_testCV = countVec.transform(x_test)\n",
    "\n",
    "#converting the vectors into array to use further\n",
    "x_train = x_trainCV.toarray()\n",
    "x_test = x_testCV.toarray()\n",
    "\n",
    "#Training and Testing on SCikit Neural Network library\n",
    "neural = MLPClassifier().fit(x_train,y_train)\n",
    "clf = SVC()\n",
    "clf.fit(x_train,y_train)\n",
    "clf.score(x_test, y_test)\n",
    "clf.predict(x_test)\n",
    "accuracy = neural.score(x_test, y_test)\n",
    "out = neural.predict(x_test)\n",
    "\n",
    "\n",
    "#Building Dictionary for Weapons\n",
    "weapons = pd.read_csv('weapons.csv',header = None, encoding = 'latin')\n",
    "weapons = weapons.iloc[:,0].tolist()\n",
    "weaponDict = {}\n",
    "for w in weapons:\n",
    "    weaponDict[w] = 1\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "outweapon = ['-' for i in range(len(corpus))]\n",
    "outperp = ['-' for i in range(len(corpus))]\n",
    "\n",
    "for i,docs in enumerate(corpus):\n",
    "    flag = 0\n",
    "    words = word_tokenize(docs)\n",
    "    newwords = word_tokenize(docs.lower())\n",
    "    tagged = pos_tag(newwords)\n",
    "    for j, w in enumerate(words):\n",
    "        perp = w\n",
    "        if perp.lower() == 'fmln':\n",
    "            outperp[i] = perp\n",
    "        weap = stemmer.stem(w)\n",
    "        for keys in weaponDict:\n",
    "            seq = difflib.SequenceMatcher(None,keys,weap)\n",
    "            correl = seq.ratio()*100\n",
    "            if correl > 90:\n",
    "                #print \"Before:\", w\n",
    "                #print \"Pos:\",tagged[j]\n",
    "                #print i\n",
    "                #print tagged[j][1]\n",
    "                if tagged[j][1] not in ['DT' ,'VBN', 'VB','VBG','VBZ', 'VBD', 'VBP']:\n",
    "                    outweapon[i] = w\n",
    "                    flag = 1\n",
    "                    #print \"After:\", w\n",
    "        if flag == 1:\n",
    "            break                    \n",
    "    #if len(outweapon[i]) == 0:\n",
    "    #    outweapon[i] = '-'\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\nNone None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\nNone None\nNone ('lopez', 14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\nNone None\nNone None\nNone None\nNone None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None ('cristiani', 48)\nNone None\nNone None\nNone ('michael', 70)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None ('mario', 7)\nNone ('juan parra del riego', -23)\nNone None\nNone None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\nNone None\nNone None\n"
     ]
    }
   ],
   "source": [
    "perp_org_list = ['-' for i in corpus]\n",
    "perp_ind_list = ['-' for i in corpus]\n",
    "\n",
    "\n",
    "for i,docs in enumerate(corpus):\n",
    "    flag = 0\n",
    "    if i > 0:\n",
    "        docs = \" \".join(docs.split('\\n')[1:])\n",
    "        \n",
    "        id_reg = re.compile('.*\\]')\n",
    "        \n",
    "        doc = []\n",
    "        for sent in sent_tokenize(docs):\n",
    "            sub_str = id_reg.sub(\"\", sent)\n",
    "            doc.append(sub_str)\n",
    "    \n",
    "        org = perp_org(doc)\n",
    "        ind = perp_ind(doc)\n",
    "        print(org, ind)\n",
    "        \n",
    "        if i > 500:\n",
    "            sys.exit()\n",
    "    # words = word_tokenize(docs)\n",
    "    # newwords = word_tokenize(docs.lower())\n",
    "    # tagged = pos_tag(newwords)\n",
    "    # for j, w in enumerate(words):\n",
    "    #     perp = w\n",
    "    #     if perp.lower() == 'fmln':\n",
    "    #         outperp[i] = perp\n",
    "    #     weap = stemmer.stem(w)\n",
    "    #     for keys in weaponDict:\n",
    "    #         seq = difflib.SequenceMatcher(None,keys,weap)\n",
    "    #         correl = seq.ratio()*100\n",
    "    #         if correl > 90:\n",
    "    #             #print \"Before:\", w\n",
    "    #             #print \"Pos:\",tagged[j]\n",
    "    #             #print i\n",
    "    #             #print tagged[j][1]\n",
    "    #             if tagged[j][1] not in ['DT' ,'VBN', 'VB','VBG','VBZ', 'VBD', 'VBP']:\n",
    "    #                 outweapon[i] = w\n",
    "    #                 flag = 1\n",
    "    #                 #print \"After:\", w\n",
    "    #     if flag == 1:\n",
    "    #         break                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Writing the Template Results\n",
    "temp = 'news.txt.templates'\n",
    "newf = open(temp, \"w\")\n",
    "for i in range(1, len(corpus)):\n",
    "    doc = corpus[i]\n",
    "    docsplit = doc.split('\\n')  \n",
    "    head = docsplit[0].split()\n",
    "    header = head[0]\n",
    "    newf.write(\"ID: {0:>16}\".format(header))\n",
    "    newf.write('\\n')    \n",
    "    x_testCV = countVec.transform([doc])\n",
    "    x_test = x_testCV.toarray()\n",
    "    out = neural.predict(x_test)\n",
    "    inc = revinc[out[0]]\n",
    "    newf.write(\"INCIDENT: {0:>16}\".format(inc))\n",
    "    newf.write('\\n')    \n",
    "    newf.write(\"WEAPON: {0:>16}\".format(outweapon[i]))\n",
    "    newf.write('\\n')    \n",
    "    newf.write(\"PERP INDIV: {0:>16}\".format('-'))\n",
    "    newf.write('\\n')    \n",
    "    newf.write(\"PERP ORG: {0:>16}\".format(outperp[i]))\n",
    "    newf.write('\\n')    \n",
    "    newf.write(\"TARGET: {0:>16}\".format('-'))\n",
    "    newf.write('\\n')    \n",
    "    newf.write(\"VICTIM: {0:>16}\".format('-'))\n",
    "    newf.write('\\n')\n",
    "    newf.write('\\n')\n",
    "newf.close()    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "\n",
    "MINISTRY CONDEMNED TODAY THE ASSASSINATION OF LEFTIST SALVADORAN LEADER\n",
    "HECTOR OQUELI AND GUATEMALAN POLITICAL LEADER GILDA FLORES.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import nsubj, VERB\n",
    "\n",
    "# Finding a verb with a subject from below â€” good\n",
    "verbs = set()\n",
    "for possible_subject in doc:\n",
    "    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
    "        verbs.add(possible_subject.head)\n",
    "\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\u001b[93m    Serving on port 5000...\u001b[0m\n    Using the 'dep' visualizer\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kbg/anaconda3/lib/python3.6/runpy.py:193: DeprecationWarning: Positional arguments to Doc.merge are deprecated. Instead, use the keyword arguments, for example tag=, lemma= or ent_type=.\n  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n    Shutting down server on port 5000.\n\n"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.serve(doc, style='dep')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
